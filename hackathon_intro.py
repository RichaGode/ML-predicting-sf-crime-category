# -*- coding: utf-8 -*-
"""hackathon-intro.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14NWWltlHo8aYAvHxj9vt38rysXE-3jbZ

# San Francisco Crime

For this hackathon, we'll be using the a database of crimes reported in San Francisco over a number of years.

This is a supervised learning task, with the goal of predicting the *category* of crime a given report will fall into given the date, police district, address, and longitude/latitude of the report.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from sklearn.svm import SVC
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('ggplot')

"""Our training data has the following structure:"""

train = pd.read_csv('hackathon_train.csv')
train.head()

"""Our test set is has the following structure:"""

test = pd.read_csv('hackathon_test.csv', index_col='Id')
test.head()

"""The goal is to predict the probability that a given report has a particular category. In order to do this, each team will submit a test results csv with an `Id` column and one column for each category of crime in the training set. The values in these columns will be predictions of the probability that a report falls into the given category."""

predictions = pd.DataFrame(
    0,
    index=test.index,
    columns=['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY',
       'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE',
       'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION',
       'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING',
       'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING',
       'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES',
       'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE',
       'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE',
       'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE',
       'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT',
       'WARRANTS', 'WEAPON LAWS']
)
predictions.head()

"""# Data Cleaning

One Hot Encoding Districts
"""

def encode_and_bind(original_dataframe, feature_to_encode):
    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])
    res = pd.concat([original_dataframe, dummies], axis=1)
    res.drop(feature_to_encode, axis=1, inplace=True)
    return(res)

train = encode_and_bind(train, "PdDistrict")
test = encode_and_bind(test, "PdDistrict")
train.head()

"""Removing latitude greater than 38"""

def latitude_cleaning(df):
    output = df[df['Y'] <=38]
    return output

train = latitude_cleaning(train)
# test = latitude_cleaning(test)

"""Adding Weekday column"""

def weekday_cleaning(df): 
  df['Weekend'] = df.DayOfWeek.isin(['Saturday', 'Sunday']) * 1
  return df

train = weekday_cleaning(train)
test = weekday_cleaning(test)

def convert_date_to_month_year_time(df=pd.DataFrame):
    df['Dates'] =  pd.to_datetime(df['Dates'])
    df['year'] = df['Dates'].dt.year.astype(int)
    df['month'] = df['Dates'].dt.month.astype(int)
    df['day'] = df['Dates'].dt.day.astype(int)
#     df['time'] = df['Dates'].dt.time
    hour = df['Dates'].dt.hour
    df['bucket'] =pd.cut(hour,[-1, 5, 13, 17, 25],labels=['Night','Morning','Afternoon','Evening'])
#     df.drop("Dates", axis=1, inplace=True)
    df = encode_and_bind(df,'bucket')
    return df

train = convert_date_to_month_year_time(train)
test = convert_date_to_month_year_time(test)

train.head()

test.head()

train.columns

"""# Analysis"""

def plot_dataframe(df):
    df.plot.bar()
    plt.rcParams["figure.figsize"] = (20,8)
    plt.show()

"""1. Crime Category Counts"""

def get_crime_plot(df):
  category_count = df[["Category","Dates"]].groupby("Category")["Dates"].nunique()
  category_count.columns = ["Category", "Counts"]
  category_count.columns
  plot_dataframe(category_count)

get_crime_plot(train)

"""2. Crime Day of the Week Counts"""

def get_crime_day_counts(df):
  df['COUNTER'] =1       #initially, set that counter to 1.
  category_day_count = df.groupby(['Category','DayOfWeek'])['COUNTER'].sum() #sum function
  category_day_count.columns=["Category", "DayOfWeek", "Counter"]
  print(category_day_count)

get_crime_day_counts(train)

#Removing useless columns
train.drop(["DayOfWeek", "Resolution", "Address", "Descript", "Dates"], axis=1, inplace=True)
test.drop(["DayOfWeek", "Address","Dates"], axis=1, inplace=True)

"""# Modeling"""

clf = SVC()
features = train.columns[1:]
clf.fit(train[features], train["Category"])
predictions = clf.predict(test[features])
test_results = pd.read_csv('hackathon_test_result.csv', index_col='Id')
count = 0
for i in range(len(predictions)):
    if predictions[i] == test_results.values[i]:
        count += 1
accuracy = count/len(predictions) * 100
print(accuracy)

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing
from sklearn import model_selection

cv = model_selection.KFold(n_splits=5, shuffle=True)
enc = preprocessing.LabelEncoder()

y = enc.fit_transform(train.Category)

train.drop(["Category"],axis=1, inplace=True)
X = train[train.columns]

est = RandomForestClassifier()

model_selection.cross_validate(est, X, y, cv=cv, return_train_score=False)

train["Category"]

"""# Scoring

The results you generate will be scored using `neg_log_loss` scoring against the known category of the test set:
"""

from sklearn.metrics import log_loss

test_results = pd.read_csv('hackathon_test_result.csv', index_col='Id')
test_results.head()

# Predicting that none of the categories are ever selected; you should be able to beat this...
log_loss(test_results, predictions, labels=predictions.columns)